import sqlite3
import faiss
import json
import datetime
import uuid
import numpy as np
import torch
import os
from openai import OpenAI
from sentence_transformers import SentenceTransformer
from flask import Flask, render_template, request, jsonify
from googleapiclient.discovery import build
from google.oauth2.service_account import Credentials

# Initialize Flask app
app = Flask(__name__)

# Setup the Google API credentials
creds = Credentials.from_service_account_file('ai-chat-443202-383c7e537494.json', 
    scopes=['https://www.googleapis.com/auth/drive.readonly',
    'https://www.googleapis.com/auth/documents.readonly'])

with open('settings.json', 'r') as file:
    settings = json.load(file)

api_key = settings["openai_api_key"]
google_folder_id = settings["google_folder_id"]

embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

# Set your OpenAI API key
client = OpenAI(api_key=api_key)

# Build the Google Drive service
drive_service = build('drive', 'v3', credentials=creds)

# Build the Google Docs service
docs_service = build('docs', 'v1', credentials=creds)

# Assuming you have a global variable to store the context
session_context = "You are a helpful assistant."

# Database setup
def initialize_database():
    conn = sqlite3.connect('documents.db')  # Creates or connects to a SQLite database
    cursor = conn.cursor()

    # Create table
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS documents (
            id TEXT PRIMARY KEY,
            name TEXT NOT NULL,
            content TEXT NOT NULL
        )
    ''')

    cursor.execute('''
        CREATE TABLE IF NOT EXISTS log (
            log_id INTEGER PRIMARY KEY AUTOINCREMENT,
            log_description TEXT NOT NULL,
            created DATETIME DEFAULT CURRENT_TIMESTAMP
        )
    ''')

    conn.commit()
    conn.close()

# Save the model weights (not typically required if you're using a pretrained model like SentenceTransformer)
def save_model_weights(model, file_path):
    torch.save(model.state_dict(), file_path)

# Load the model weights
def load_model_weights(file_path, model):
    model.load_state_dict(torch.load(file_path, weights_only=True))
    model.eval()

def save_to_log(log_description):
    """
    Saves a log entry into the log table.

    Parameters:
        log_description (str): The description of the log entry.
    """
    conn = sqlite3.connect('documents.db')  # Connects to the SQLite database
    cursor = conn.cursor()

    # Insert a new log entry
    cursor.execute('''
        INSERT INTO log (log_description)
        VALUES (?)
    ''', (log_description,))

    conn.commit()
    conn.close()

# Save document to database
def save_document_to_db(doc_id, doc_name, doc_content):
    conn = sqlite3.connect('documents.db')
    cursor = conn.cursor()

    # Insert or replace document
    cursor.execute('''
        INSERT OR REPLACE INTO documents (id, name, content)
        VALUES (?, ?, ?)
    ''', (doc_id, doc_name, doc_content))

    conn.commit()
    conn.close()

# Retrieve documents (for debugging or use)
def fetch_all_documents():
    conn = sqlite3.connect('documents.db')
    cursor = conn.cursor()

    # Query all documents
    cursor.execute('SELECT id, name, content FROM documents')
    documents = cursor.fetchall()

    conn.close()
    return documents

def save_docs_to_database(drive_service, folder_id):
    """
    Extracts content from Google Docs in a folder and saves them to an SQLite database.
    """
    # Initialize database
    initialize_database()

    # Get documents and their content
    all_docs = list_files_in_folder(drive_service, folder_id)
    for doc in all_docs:
        print(f"Processing document: {doc['name']} (ID: {doc['id']})")
        content = extract_google_doc_content(drive_service, doc['id'])

        # Save to database
        save_document_to_db(doc['id'], doc['name'], content)
    print("All documents saved to database.")

def create_embeddings(documents):
    """
    Create embeddings for the documents' content.
    An embedding is a numerical representation of data (like text, images, or audio) in a continuous vector space. In the context of text, embeddings are vectors that represent the semantic meaning of the text. They are typically generated by machine learning models trained on large datasets to understand relationships between words, sentences, or documents.
    """
    contents = [doc[2] for doc in documents]  # Extract the 'content' field
    embeddings = [embedding_model.encode(content) for content in contents]
    return embeddings


def setup_faiss_index(embeddings):
    """
    Build a FAISS index for document retrieval.
    """
    dimension = len(embeddings[0])  # Get the dimension of the embeddings

    # Convert embeddings list to a NumPy array of type float32
    embeddings = np.array(embeddings).astype('float32')

    index = faiss.IndexFlatL2(dimension)
    index.add(embeddings)  # Add document embeddings to the FAISS index
    return index

def get_relevant_chunks(query, index, documents, k=10):
    """
    Retrieve top-k relevant documents for a given query.
    """
    # Generate embedding for the query
    query_embedding = embedding_model.encode(query)

    # Convert query_embedding to a NumPy array of type float32
    query_embedding = np.array(query_embedding, dtype='float32').reshape(1, -1)

    # Search for top-k matches in the FAISS index
    _, top_indices = index.search(query_embedding, k)

    # Retrieve the 'content' field of the top-k documents
    relevant_chunks = [documents[i][2] for i in top_indices[0]]
    return relevant_chunks

# Function to list files in a Google Drive folder
def list_files_in_folder(drive_service, folder_id):
    """
    Recursively lists all Google Docs files in a folder and its subfolders.
    Returns a list of files with their names and IDs.
    """
    print("Getting all the files in folder: " + folder_id)

    all_files = []

    # Query for Google Docs in the current folder
    query = f"'{folder_id}' in parents and mimeType = 'application/vnd.google-apps.document'"
    results = drive_service.files().list(q=query, pageSize=100).execute()
    files = results.get('files', [])

    # Add the files found in the current folder to the list
    for file in files:
        all_files.append({'name': file['name'], 'id': file['id']})

    # Query for subfolders in the current folder
    query_folders = f"'{folder_id}' in parents and mimeType = 'application/vnd.google-apps.folder'"
    subfolders = drive_service.files().list(q=query_folders, pageSize=100).execute().get('files', [])

    # Recursively search each subfolder
    for subfolder in subfolders:
        subfolder_files = list_files_in_folder(drive_service, subfolder['id'])
        all_files.extend(subfolder_files)

    return all_files

def extract_google_doc_content(drive_service, doc_id):
    """
    Extracts the text content from a Google Doc by its ID.
    """
    document = docs_service.documents().get(documentId=doc_id).execute()
    content = ""

    # Extract text from the document
    for element in document.get('body', {}).get('content', []):
        if 'paragraph' in element:
            for text_run in element['paragraph'].get('elements', []):
                content += text_run.get('textRun', {}).get('content', '')
    return content

def summarize_documents(documents):
    summarized_docs = []
    for doc in documents:

        print("summarize_documents...")
        completion = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "Kamu adalah asisten menggunakan bahasa indonesia."},
                {"role": "user", "content": f"gunakan bahasa indonesia, rangkum dokumen berikut:\n{doc[2]}"}
            ],
            max_tokens=3000  # Adjust as needed
        )

        summary = completion.choices[0].message.content
        summarized_docs.append(f"Document: {doc[1]}\nSummary: {summary}")
    return summarized_docs

def filter_relevant_documents(user_input, documents):
    relevant_docs = []
    for doc in documents:
        if any(keyword in doc[2].lower() for keyword in user_input.lower().split()):
            relevant_docs.append(doc)
    return relevant_docs

def split_into_chunks(text, max_tokens=1000):
    import tiktoken
    enc = tiktoken.encoding_for_model("gpt-4")
    tokens = enc.encode(text)
    for i in range(0, len(tokens), max_tokens):
        yield enc.decode(tokens[i:i + max_tokens])

import sqlite3

def get_session_context(session_id):
    """
    Fetch the session context from the database.
    Parameters:
        session_id (str): The unique ID of the session.
    Returns:
        str: The session context if found, otherwise an empty string.
    """
    conn = sqlite3.connect('documents.db')
    cursor = conn.cursor()

    # Fetch session context
    cursor.execute("SELECT context FROM session WHERE id = ?", (session_id,))
    row = cursor.fetchone()

    # Return context or initialize if not found
    session_context = row[0] if row else ""  # If no row is found, return an empty string
    conn.close()
    return session_context


def update_session_context(session_id, new_context):
    """
    Update the session context in the database, including the created timestamp for new sessions.
    
    Parameters:
        session_id (str): The unique ID of the session.
        new_context (str): The updated session context.
    """
    conn = sqlite3.connect('documents.db')
    cursor = conn.cursor()

    # Check if the session exists
    cursor.execute("SELECT id FROM session WHERE id = ?", (session_id,))
    if cursor.fetchone():
        # Update the context for an existing session
        cursor.execute("UPDATE session SET context = ? WHERE id = ?", (new_context, session_id))
    else:
        # Insert a new session with the current timestamp
        created = datetime.now().strftime('%Y-%m-%d %H:%M:%S')  # Generate current timestamp
        cursor.execute("INSERT INTO session (id, context, created) VALUES (?, ?, ?)", 
                       (session_id, new_context, created))

    conn.commit()
    conn.close()

# Define route for the homepage
@app.route('/')
def home():
    return render_template('index.html')

# Route for chatbot responses
@app.route('/chat', methods=['POST'])
def chat():
    global session_context
    user_input = request.form['user_input']  # Get user input from form
    session_id = request.form['session_id']  # Get user input from form
    folder_id = google_folder_id  # Replace with your folder ID

    try:
        save_to_log(session_id + " " +user_input)

        model_weights_path = "model_weights.pth"

        if os.path.exists(model_weights_path):
            save_to_log("loading model_weights")
            checkpoint = torch.load(model_weights_path, weights_only=True)

            # Check if the checkpoint has the expected keys
            if 'model_state_dict' in checkpoint and 'embeddings' in checkpoint:
                embedding_model.load_state_dict(checkpoint['model_state_dict'])
                embedding_model.eval()
                embeddings = checkpoint['embeddings']
            else:
                raise KeyError("Checkpoint file is missing 'model_state_dict' or 'embeddings'")

        else:
            # Step 1: Fetch documents
            documents = fetch_all_documents()

            # Step 2: Generate embeddings
            embeddings = create_embeddings(documents)
            serializable_embeddings = [embedding.tolist() for embedding in embeddings]

            # Save model weights and embeddings
            torch.save({
                'model_state_dict': embedding_model.state_dict(),
                'embeddings': serializable_embeddings
            }, model_weights_path)


        # Save to log
        # save_to_log("embeddings: " + json.dumps({"embedding": serializable_embeddings}))

        # Step 3: Setup FAISS index
        index = setup_faiss_index(embeddings)
        index_metadata = {
            "ntotal": index.ntotal,  # Number of vectors in the index
            "d": index.d             # Dimensionality of the vectors
        }
        index_metadata_json = json.dumps(index_metadata)
        
        save_to_log("index metadata: " + index_metadata_json)

        timestamp = datetime.datetime.now().strftime("%Y%m%d%H%M%S")
        index_file_path = f"faiss_index_{timestamp}.idx"

        # Save the FAISS index
        # faiss.write_index(index, index_file_path)

        # Step 4: Retrieve relevant chunks
        relevant_chunks = get_relevant_chunks(user_input, index, documents, k=10)
        save_to_log("relevant_chunks: "+ json.dumps(relevant_chunks))

        # filtered_docs = filter_relevant_documents(user_input, documents)
        # summarized_texts = summarize_documents(filtered_docs[:10])  # Process only top 10 relevant docs
        # all_docs_text = "\n\n".join(summarized_texts)

        # Get session context from database
        #session_context = get_session_context(session_id)

        # Add the documents content if it's a new session
        #if not session_context.endswith(all_docs_text):
        #    session_context += f"\ndokumentasi aplikasi bukutansi adalah sebagai berikut: {all_docs_text}"

        # Append the user's question to the context
        #session_context += f"\nUser: {user_input}"

        # Save the updated session context back to the database
        #update_session_context(session_id, session_context)

        completion = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "Kamu adalah asisten menggunakan bahasa indonesia."},
                {"role": "user", "content": f"gunakan bahasa indonesia, dan format jawaban dalam html sehingga baris baru menggunakan <br> dan sub title menggunakan font tebal <b>. Informasi relevant: {relevant_chunks}. pertanyaan saya berhubungan dengan bukutansi: {user_input}. Jika tidak ditemukan jawabannya maka ucapkan maaf dan minta agar menghubungi wa +628112937076"}
            ],
            max_tokens=1000  # Adjust as needed
        )
        
        message_content = completion.choices[0].message.content

        # Return the response to the user in JSON format
        return jsonify({'response': message_content})


    except Exception as e:
        save_to_log(str(e))     
        return jsonify({'response': f"Error: {str(e)}"})

# Run the Flask app
if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000,debug=True)
